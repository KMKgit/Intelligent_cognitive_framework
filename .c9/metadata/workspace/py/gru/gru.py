{"changed":true,"filter":false,"title":"gru.py","tooltip":"/py/gru/gru.py","value":"import numpy as np\nimport pandas as pd\nimport sys\nimport json\nimport time\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, SimpleRNN, GRU\nfrom keras.layers import Dense, Dropout, Activation\nfrom os import getcwd, environ, listdir, mkdir, path\nfrom keras.optimizers import rmsprop\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import load_model\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.externals import joblib\n\nclass gru(object):\n    \n    def setColumns(self):\n        \n        self.PATH = getcwd()\n        f = open(self.PATH + '/data/' + sys.argv[1] + '/' + sys.argv[1] + '.info', 'r')\n        p = open(self.PATH + '/data/' + sys.argv[1] + '/' + sys.argv[1] + '.param', 'r')\n        self.columns = f.read().splitlines()\n      \n        t = json.load(p)\n        self.label_name = str(t['label_name'])\n        self.learning_rate = float(t['learning_rate'])\n        self.batch_size = int(t['batch_size'])\n        self.hidden_layer = int(t['hidden_layer'])\n        self.hidden_unit = t['hidden_unit']\n        self.dropout = t['dropout']\n        self.epoch = int(t['epoch'])\n        #self.timesteps = int(t['timesteps'])\n        \n        #self.learning_rate = 0.01\n        #self.batch_size = 50\n        #self.hidden_layer = 2\n        #self.hidden_unit = [32, 32]\n        #self.dropout = [0.5, 0.5]\n        #self.epoch = 3\n        self.timesteps = 1\n        \n        self.in_activation = \"softmax\"\n        self.loss_function = \"categorical_crossentropy\"\n        \n    def pre_processing_for_data(self, data_file_path):\n      \n        #read data\n        data_pd = pd.read_csv(data_file_path)\n        #assign features \n        features_pd = data_pd\n        #features_pd = features_pd.drop('label',1)\n       \n        if self.label_name in features_pd:\n            features_pd = features_pd.drop(self.label_name,1)\n        \n        self.data_dim = len(features_pd.columns)\n        self.samples = len(features_pd)\n        #crate input data\n        X_val = np.array(features_pd)\n        adjust_offset_value = len(X_val)-(len(X_val) % self.timesteps)\n        X_val = X_val[0:adjust_offset_value]\n        X_val = X_val.reshape(-1, self.timesteps, self.data_dim)\n        #assign label\n        #label_pd = data_pd['label']\n        if self.label_name is not None:\n            if self.label_name in data_pd:\n                label_pd = data_pd[self.label_name]\n                #calculate label list\n                label_list = list(set(np.reshape(label_pd.values,(-1,))))\n                #create output data\n                #Y_val = []\n                #loop for number of data\n                #for count, i  in enumerate(label_pd):\n                #    idx = find_matching_index(label_list,i)\n                #    Y_val.append(np.reshape(vectorized_Y_data(idx,len(label_list)),(-1,)))\n                Y_val = np.array(label_pd)\n                Y_val = to_categorical(Y_val)\n            else:\n                Y_val = 0\n        else:\n            Y_val = 0\n      \n        #print(np.shape(X_val))\n        #print(np.shape(Y_val))\n        return X_val, Y_val\n      \n    \n    def training(self):\n        #init network configuration\n        self.setColumns()\n        #preprocessing data\n        X_train, Y_train = self.pre_processing_for_data(str(self.PATH + '/data/' + sys.argv[1] + '/' + sys.argv[1] + '.csv'))\n        \n        #create model\n        model = Sequential()\n        #input layer\n        model.add(GRU(output_dim = int(self.hidden_unit[0]), \n                    return_sequences=True, \n                    input_shape=(self.timesteps, self.data_dim)))\n        #hidden layer\n        for i in range(0, self.hidden_layer):\n            if i == self.hidden_layer-1:\n                model.add(GRU(int(self.hidden_unit[i])))\n            else:\n                model.add(GRU(int(self.hidden_unit[i]), return_sequences=True))\n                model.add(Dropout(float(self.dropout[i])))\n        #output layer\n        model.add(Dense(len(Y_train[0]), activation=self.in_activation))\n        #set cost-function, optimizser, metrics\n        model.compile(loss=self.loss_function, optimizer=rmsprop(lr = self.learning_rate), metrics=['accuracy'])\n        #do training\n        model.fit(X_train, Y_train, batch_size=self.batch_size, nb_epoch=self.epoch, validation_data=(X_train, Y_train))\n        #save model\n        out = open(self.PATH + '/data/' + sys.argv[1] + '/' + sys.argv[1] + '.out', 'w')\n        P =  model.predict_classes(X_train, verbose=0)\n        score = model.evaluate(X_train, Y_train, verbose=0)\n        model_name = 'train_'+time.strftime(\"%Y%m_%d_%H_%M\", time.localtime())\n        \n        json.dump({'ntb':\n                    {\n                        'model_name' : model_name,\n                        'samples' : self.samples,\n                        'score' : score[0],\n                        'accuracy' : score[1],\n                        #'recall_score' : recall_score(P, Y_train, average='weighted'), # it's not working because of multi-dimension\n                        #'precision_score' : precision_score(P, Y_train, average='weighted') # it's not working becuse of multi-dimension\n                \n                    }\n                  ,\n        \n                  }, out, separators=(',',':'))\n        \n        if not path.exists(self.PATH + '/data/' + sys.argv[1] + '/' +  model_name):\n            mkdir(self.PATH + '/data/' + sys.argv[1] + '/' + model_name)\n        model.save(self.PATH + '/data/' + sys.argv[1] + '/' + model_name + '/' + 'model' + '.h5')  \n        #model.save('./test_py/gru/weight.h5')\n        del model\n        out.close()\n\n\n    def test(self):\n        #init network configuration\n        self.setColumns()\n        #preprocessing data\n        X_test, Y_test = self.pre_processing_for_data(str(self.PATH + '/data/' + sys.argv[1] + '/' + sys.argv[1] + '.csv'))\n        #load latest train model\n        f = open(self.PATH + '/data/' + sys.argv[1] + '/' + sys.argv[1] + '.out', 'r')\n        t = json.load(f)\n        model_path = self.PATH + '/data/' + sys.argv[1] + '/' + sys.argv[3] + '/' + 'model.h5'\n        model = load_model(model_path)\n        \n        #save test result\n        #test = open(self.PATH + '/data/' + sys.argv[1]  + '/test/' + 'test.test', 'w')\n        test = open(self.PATH + '/data/' + sys.argv[1] + '/test/' + sys.argv[2] + '.test', 'w')\n        P = model.predict_classes(X_test, verbose=0)\n        score = model.evaluate(X_test, Y_test, verbose=0)\n        \n        json.dump({'ntb':\n                    {\n                      'samples' : self.samples,\n                      'score' : score[0],\n                      'accuracy' : score[1]\n                    }\n                  ,\n                  'tb': \n                    {\n                    }\n                  }, test, separators=(',',':'))\n        del model\n        test.close()\n\n\n    def request(self):\n        \n        #initialize data\n        self.setColumns()\n        #configure data path\n        X_request, _ = self.pre_processing_for_data(str(self.PATH + '/data/' + sys.argv[1] + '/request/' + sys.argv[2] + '.csv'))\n        #load latest train model\n        f = open(self.PATH + '/data/' + sys.argv[1] + '/' + sys.argv[1] + '.out', 'r')\n        t = json.load(f)\n        model_path = self.PATH + '/data/' + sys.argv[1] + '/' + sys.argv[3] + '/' + 'model.h5'\n        model = load_model(model_path)\n        \n        #save requset result\n        #argv[2]\n        req = open(self.PATH + '/data/' + sys.argv[1]  + '/request/' + sys.argv[2] +'.req', 'w')\n        P = model.predict_classes(X_request, verbose=0)\n        #score = model.evaluate(X_request, Y_request, verbose=0)\n        \n        prediction_array = []\n        for i in P:\n            prediction_array.append(i)\n            \n        json.dump({'ntb':\n                    {\n                      'samples' : self.samples,\n                    }\n                  ,\n                  'tb': \n                    {\n                        'prediction' : prediction_array\n                    }\n                  }, req, separators=(',',':'))\n        del model\n        req.close()\n\ndef find_matching_index(src, dst):\n    for i in range(0, len(src)):\n        if src[i] == dst:\n            return i\n    return -1\n\ndef vectorized_Y_data(j,label_num):\n    e = np.zeros((label_num, 1))\n    e[j] = 1.0\n    return e[:]\n\ndef main():\n    mygru = gru()\n    mygru.training()\n    #mygru.test()\n    #mygru.request()\n    \nif __name__ == \"__main__\":\n    main()","undoManager":{"mark":-2,"position":0,"stack":[[{"start":{"row":133,"column":0},"end":{"row":134,"column":0},"action":"remove","lines":["        if not path.exists(self.PATH + '/data/' + sys.argv[1] + '/h5'):",""],"id":20}]]},"ace":{"folds":[],"scrolltop":3019.5,"scrollleft":0,"selection":{"start":{"row":190,"column":8},"end":{"row":190,"column":8},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":18,"state":"start","mode":"ace/mode/python"}},"timestamp":1485854288904}